{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9 Machine Learning With Spark ML\n",
    "As the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients. It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
    "1. Load the data.\n",
    "2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n",
    "3. Train a model to predict the target variable (risk of `default`).\n",
    "  - Employ three different models (logistic regression, decision tree, and random forest).\n",
    "  - Compare the models' performances (e.g., AUC).\n",
    "  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n",
    "4. What more would you do with this data? Anything to help you devise a better solution?\n",
    "\n",
    "---\n",
    "# 1. Get the data\n",
    "We start by loading the dataset. we infer column types automatically by reading the filewith `inferSchema` to true. The `header` option will read the columns' name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 23 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val default = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"data/ccdefault.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "here we analyse the data by obtaining the statistical summary of the attributes. We attempt to find the correlation between the different features. In the end, we combine 18 out of the 24 features in 3 groups of 6 to generate 3 new attribues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- DEFAULT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Long = 30000\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|DEFAULT|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|      1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|      1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|      0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|      0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|      0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with age more than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Long = 18987\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default.filter(\"age > 30\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "A summary of the table statistics for the attributes `age`, `sex`, `education`, and `limit_bal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|              age|               sex|         education|         limit_bal|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|            30000|             30000|             30000|             30000|\n",
      "|   mean|          35.4855|1.6037333333333332|1.8531333333333333|167484.32266666667|\n",
      "| stddev|9.217904068090155|0.4891291960902602|0.7903486597207269|129747.66156720246|\n",
      "|    min|               21|                 1|                 0|             10000|\n",
      "|    max|               79|                 2|                 6|           1000000|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "default.describe(\"age\", \"sex\", \"education\", \"limit_bal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Correlation among attributes\n",
    "Correlation between the attributes `age`, `sex`, `education`, and `limit_bal`,by computing the standard correlation coefficient (Pearson) between every pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+--------------------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|DEFAULT|            features|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+--------------------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|      1|[20000.0,2.0,2.0,...|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|      1|[120000.0,2.0,2.0...|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|      0|[90000.0,2.0,2.0,...|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|      0|[50000.0,2.0,2.0,...|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|      0|[50000.0,1.0,2.0,...|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_6e03b13ab469\n",
       "defaultAttrs: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 24 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\")).setOutputCol(\"features\")\n",
    "\n",
    "val defaultAttrs = va.transform(default)\n",
    "\n",
    "defaultAttrs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficient:\n",
      " 1.0                   0.024755235111645853  -0.2191606982292233   ... (5 total)\n",
      "0.024755235111645853  1.0                   0.01423193616219367   ...\n",
      "-0.2191606982292233   0.01423193616219367   1.0                   ...\n",
      "-0.10813941027800818  -0.03138884007083411  -0.14346434041145634  ...\n",
      "0.14471279755736938   -0.09087364652720994  0.17506066148814436   ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Matrix\n",
       "import org.apache.spark.ml.stat.Correlation\n",
       "import org.apache.spark.sql.Row\n",
       "coeff: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0                   0.024755235111645853  -0.2191606982292233   ... (5 total)\n",
       "0.024755235111645853  1.0                   0.01423193616219367   ...\n",
       "-0.2191606982292233   0.01423193616219367   1.0                   ...\n",
       "-0.10813941027800818  -0.03138884007083411  -0.14346434041145634  ...\n",
       "0.14471279755736938   -0.09087364652720994  0.17506066148814436   ...\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Matrix\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val Row(coeff: Matrix) = Correlation.corr(defaultAttrs, \"features\").head\n",
    "\n",
    "println(s\"The standard correlation coefficient:\\n ${coeff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Combine and make new attributes\n",
    "We chose to combine the mopnthly payment parameters and to average them. We generate 3 new features, `repayment`, `billStatement`, and `payment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|          repayment|     billStatement|           payment|\n",
      "+-------------------+------------------+------------------+\n",
      "|-0.3333333333333333|            1284.0|114.83333333333333|\n",
      "|                0.5|2846.1666666666665| 833.3333333333334|\n",
      "|                0.0|16942.166666666668|1836.3333333333333|\n",
      "|                0.0|38555.666666666664|            1398.0|\n",
      "|-0.3333333333333333|18223.166666666668|            9841.5|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "repayCol: Array[org.apache.spark.sql.Column] = Array(PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6)\n",
       "averageRepay: org.apache.spark.sql.Column = (((((((0 + PAY_0) + PAY_2) + PAY_3) + PAY_4) + PAY_5) + PAY_6) / 6)\n",
       "default1: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 24 more fields]\n",
       "billCol: Array[org.apache.spark.sql.Column] = Array(BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6)\n",
       "averageBill: org.apache.spark.sql.Column = (((((((0 + BILL_AMT1) + BILL_AMT2) + BILL_AMT3) + BILL_AMT4) + BILL_AMT5) + BILL_AMT6) / 6)\n",
       "default2: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 25 more fields]\n",
       "payCol: Array[org.apache.spark.sql.Column] = Array(PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n",
       "averagePay..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val repayCol = Array(col(\"PAY_0\"), col(\"PAY_2\"), col(\"PAY_3\"), col(\"PAY_4\"), col(\"PAY_5\"), col(\"PAY_6\"))\n",
    "val averageRepay = repayCol.foldLeft(lit(0)){(x, y) => x+y}/repayCol.length\n",
    "val default1 = default.withColumn(\"repayment\", averageRepay)\n",
    "\n",
    "\n",
    "val billCol = Array(col(\"BILL_AMT1\"), col(\"BILL_AMT2\"), col(\"BILL_AMT3\"), col(\"BILL_AMT4\"), col(\"BILL_AMT5\"), col(\"BILL_AMT6\"))\n",
    "val averageBill = billCol.foldLeft(lit(0)){(x, y) => x+y}/billCol.length\n",
    "val default2 = default1.withColumn(\"billStatement\", averageBill)\n",
    "\n",
    "val payCol = Array(col(\"PAY_AMT1\"), col(\"PAY_AMT2\"), col(\"PAY_AMT3\"), col(\"PAY_AMT4\"), col(\"PAY_AMT5\"), col(\"PAY_AMT6\"))\n",
    "val averagePay = payCol.foldLeft(lit(0)){(x, y) => x+y}/payCol.length\n",
    "val defaultExtra = default2.withColumn(\"payment\", averagePay)\n",
    "\n",
    "defaultExtra.select(\"repayment\", \"billStatement\", \"payment\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Preparing the data for Machine Learning algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renamedDefault: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 23 more fields]\n",
       "filteredDefault: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val renamedDefault = default.withColumnRenamed(\"DEFAULT\", \"label\")\n",
    "\n",
    "val filteredDefault =renamedDefault.drop(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel: String = label\n",
       "colNum: Array[String] = Array(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// label columns\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// numerical columns\n",
    "val colNum = filteredDefault.columns.filter(_ != colLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuse attributes\n",
    "### Data cleaning\n",
    "The dataset does not have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LIMIT_BAL,0)\n",
      "(SEX,0)\n",
      "(EDUCATION,0)\n",
      "(MARRIAGE,0)\n",
      "(AGE,0)\n",
      "(PAY_0,0)\n",
      "(PAY_2,0)\n",
      "(PAY_3,0)\n",
      "(PAY_4,0)\n",
      "(PAY_5,0)\n",
      "(PAY_6,0)\n",
      "(BILL_AMT1,0)\n",
      "(BILL_AMT2,0)\n",
      "(BILL_AMT3,0)\n",
      "(BILL_AMT4,0)\n",
      "(BILL_AMT5,0)\n",
      "(BILL_AMT6,0)\n",
      "(PAY_AMT1,0)\n",
      "(PAY_AMT2,0)\n",
      "(PAY_AMT3,0)\n",
      "(PAY_AMT4,0)\n",
      "(PAY_AMT5,0)\n",
      "(PAY_AMT6,0)\n"
     ]
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "for (c <- colNum) {\n",
    "    println(c, filteredDefault.filter(filteredDefault(c).isNull || filteredDefault(c) === \"\" || filteredDefault(c).isNaN).count())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Here we standardize the values of the attributes so that the resulting distribution has unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|       featuresscale|              scaled|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|[20000.0,2.0,2.0,...|[0.15414535998894...|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[120000.0,2.0,2.0...|[0.92487215993365...|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[90000.0,2.0,2.0,...|[0.69365411995024...|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[50000.0,2.0,2.0,...|[0.38536339997235...|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[50000.0,1.0,2.0,...|[0.38536339997235...|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e6a758dc86ee\n",
       "featuredDefault: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 23 more fields]\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_28e53d6ab527\n",
       "scaledDefault: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 24 more fields]\n",
       "res43: Array[String] = Array(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, label, featuresscale, scaled)\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(colNum).setOutputCol(\"featuresscale\")\n",
    "val featuredDefault = va.transform(filteredDefault) \n",
    "val scaler = new StandardScaler().setInputCol(\"featuresscale\").setOutputCol(\"scaled\")\n",
    "val scaledDefault = scaler.fit(featuredDefault).transform(featuredDefault)\n",
    "\n",
    "scaledDefault.show(5)\n",
    "scaledDefault.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|label|       featuresscale|              scaled|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|    1|[20000.0,2.0,2.0,...|[0.15414535998894...|\n",
      "|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|    1|[120000.0,2.0,2.0...|[0.92487215993365...|\n",
      "|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|    0|[90000.0,2.0,2.0,...|[0.69365411995024...|\n",
      "|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|    0|[50000.0,2.0,2.0,...|[0.38536339997235...|\n",
      "|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|    0|[50000.0,1.0,2.0,...|[0.38536339997235...|\n",
      "+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
       "numPipeline: org.apache.spark.ml.Pipeline = pipeline_d197a4463891\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a584609d8436\n",
       "newDefault: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 24 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "\n",
    "val numPipeline = new Pipeline().setStages(Array(va, scaler))\n",
    "val pipeline = new Pipeline().setStages(Array(numPipeline))\n",
    "val newDefault = pipeline.fit(filteredDefault).transform(filteredDefault)\n",
    "newDefault.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the `dataset` to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.15414535998894...|    1|\n",
      "|[0.92487215993365...|    1|\n",
      "|[0.69365411995024...|    0|\n",
      "|[0.38536339997235...|    0|\n",
      "|[0.38536339997235...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va2: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7ae4f5490324\n",
       "dataset: org.apache.spark.sql.DataFrame = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va2 = new VectorAssembler().setInputCols(Array(\"scaled\")).setOutputCol(\"features\")\n",
    "val dataset = va2.transform(newDefault).select(\"features\", \"label\")\n",
    "\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Make a model\n",
    "Here we going to implement four different regression models:\n",
    "* Linear regression model\n",
    "* Decission tree regression\n",
    "* Random forest regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Logistic regression model\n",
    "Now, train a Linear Regression model using the `LogisticRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `binarySummary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 FPR|                 TPR|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|                 0.0|\n",
      "|0.003705692803437...|0.032003012048192774|\n",
      "|0.006874328678839957| 0.06588855421686747|\n",
      "|0.009828141783029001| 0.10052710843373494|\n",
      "|0.013480128893662728| 0.13271837349397592|\n",
      "|0.016970998925886143| 0.16547439759036145|\n",
      "|0.020461868958109558| 0.19823042168674698|\n",
      "|0.024543501611170783|  0.2289156626506024|\n",
      "| 0.02916219119226638|  0.2577183734939759|\n",
      "| 0.03431793770139635| 0.28463855421686746|\n",
      "|   0.040171858216971| 0.30911144578313254|\n",
      "| 0.04505907626208378|  0.3369728915662651|\n",
      "| 0.05080558539205156|  0.3618222891566265|\n",
      "|0.057518796992481205|  0.3832831325301205|\n",
      "| 0.06439312567132116|  0.4041792168674699|\n",
      "| 0.07234156820622986| 0.42131024096385544|\n",
      "| 0.07986036519871106|  0.4399472891566265|\n",
      "| 0.08909774436090226| 0.45256024096385544|\n",
      "|  0.0985499462943072| 0.46442018072289154|\n",
      "| 0.10735767991407089|  0.4787274096385542|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Coefficients: [-0.08059831134684567,-0.008929820397429742,-0.07003722420825702,-0.02372100541944413,0.09703499056705102,0.6415358557538792,0.13630389897718584,0.09029386347983337,0.01371619110279084,0.03664942273093899,0.00975536457877031,-0.08733981784905516,-0.03877332734528022,-0.01692947615654705,0.009225419537988269,0.025140969895007442,0.03224074033447387,-0.12493433318225211,-0.16066490551409018,-0.06970507919807174,-0.051598283886307175,-0.06280888460045131,-0.028328845347666033] Intercept: -1.207679040357071\n",
      "AreaUnderROC: 0.7195107059548613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_cf7792d896b2\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_cf7792d896b2, numClasses = 2, numFeatures = 23\n",
       "trainingSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@3d213494\n",
       "roc: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "// train the model\n",
    "val lr = new LogisticRegression().setMaxIter(10)\n",
    "val lrModel = lr.fit(trainSet)\n",
    "val trainingSummary = lrModel.binarySummary\n",
    "\n",
    "val roc = trainingSummary.roc\n",
    "roc.show()\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "println(s\"AreaUnderROC: ${trainingSummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|(23,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(23,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(23,[0,1,2,3,4,5,...|\n",
      "|       0.0|    0|(23,[0,1,2,3,4,5,...|\n",
      "|       0.0|    1|(23,[0,1,2,3,4,5,...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AreaUnderROC on test data = 0.5982478105592436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 3 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_4f6a0b944620\n",
       "roc: Double = 0.5982478105592436\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"AreaUnderROC on test data = $roc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Decision tree regression\n",
    "Repeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    1|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    1|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    1|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    1|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.2987012987012987|    0|(23,[0,1,2,3,4,5,...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "AreaUnderROC on test data = 0.7479485933575502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_d219f4d6059b\n",
       "dtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel (uid=dtr_d219f4d6059b) of depth 5 with 63 nodes\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_68400c51e586\n",
       "roc: Double = 0.7479485933575502\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val dt = new DecisionTreeRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val dtModel = dt.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(10)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"AreaUnderROC on test data = $roc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+--------------------+\n",
      "|         prediction|label|            features|\n",
      "+-------------------+-----+--------------------+\n",
      "|0.28602066035377427|    0|(23,[0,1,2,3,4,5,...|\n",
      "| 0.2922952491960832|    0|(23,[0,1,2,3,4,5,...|\n",
      "| 0.2922952491960832|    1|(23,[0,1,2,3,4,5,...|\n",
      "| 0.2922952491960832|    0|(23,[0,1,2,3,4,5,...|\n",
      "|0.25418748520266976|    1|(23,[0,1,2,3,4,5,...|\n",
      "+-------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "AreaUnderROC on test data = 0.7668859344670713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_2fded48f4edf\n",
       "rfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel (uid=rfr_2fded48f4edf) with 20 trees\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_e1de9b771033\n",
       "roc: Double = 0.7668859344670713\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rf = new RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "// train the model\n",
    "val rfModel = rf.fit(trainSet)\n",
    "\n",
    "// make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// select (prediction, true label) and compute test error\n",
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").setRawPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "val roc = evaluator.evaluate(predictions)\n",
    "println(s\"AreaUnderROC on test data = $roc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The models we used\n",
    "\n",
    "We tested three different models to predict default: Logistic Regression, Desicion Tree and Random Forest. The results were as expected, they were improving in this respective order. We used AUC to compare the different approaches. The distribution of the dataset was not equal so the result depends largely on the test set that is chosen. Logistic Regression's linear boundaries generalizes better and have less chance of overfitting compared to Decision Trees, but also is less accurate. Decision Trees are simple and explainable models and already perform quite well. But Random Forests will always yield better accuracy and is more robust as it's constructed of multiple decision trees. \n",
    "\n",
    "AreaUnderROC for test sets:\n",
    "\n",
    "Logistic Regression: 0.5982478105592436\n",
    "\n",
    "Decision Tree: 0.7479485933575502\n",
    "\n",
    "Random Forest: 0.7668859344670713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given attributes are relevant with the problem of financial default, and all the models achieved an accuracy of over 60%.\n",
    "The models could become more accurate if we obtained more historical data and would train on more aggregate features. More granular data for parameters like education would also help to improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
