{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "l2f.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DiXQli1OiamP"
      },
      "source": [
        "### Course: ID2223\n",
        "### Group: Nicolas Essipova, Peter Lakatos, Marios Chatiras\n",
        "\n",
        "# Task 1: Inception v1 Implementation\n",
        "\n",
        "Approach involves creating the inception module prior to building out the inception architecture itself, adhering to the structure outlined by the paper. Everything is implemented in TensorFlow 2.0 and Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "huVDnzRrcQ3V",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WTlPsefIrYQs",
        "colab": {}
      },
      "source": [
        "# Setting up the input and initializers for our model,\n",
        "# per what was used in the paper.\n",
        "\n",
        "input_layer = layers.Input(shape=(224, 224, 3))\n",
        "kernel_init = tf.keras.initializers.RandomUniform(-1, 1)\n",
        "bias_init = tf.keras.initializers.Constant(value=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxtXSzCHkq4F"
      },
      "source": [
        "# Defining the Inception Module\n",
        "![alt text](https://i.imgur.com/coskhAk.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YukSsa17kV-a",
        "colab": {}
      },
      "source": [
        "def inception_module(x, # previous layer\n",
        "                     filters_1x1,\n",
        "                     filters_3x3_reduce,\n",
        "                     filters_3x3,\n",
        "                     filters_5x5_reduce,\n",
        "                     filters_5x5,\n",
        "                     filters_pool_proj,\n",
        "                     name=None):\n",
        "    \n",
        "    # First path\n",
        "    conv_1x1 = layers.Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    \n",
        "    # Second path\n",
        "    conv_3x3 = layers.Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_3x3 = layers.Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
        "\n",
        "    # Third path\n",
        "    conv_5x5 = layers.Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_5x5 = layers.Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
        "\n",
        "    # Fourth path\n",
        "    pool_proj = layers.MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = layers.Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
        "\n",
        "    # Filter concatenation\n",
        "    output = layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t7q6b_BAliFO"
      },
      "source": [
        "# Constructing GoogLeNet\n",
        "\n",
        "We will now construct the model according to the final topology that is presented in the paper. Our constructed model will look exactly like this:\n",
        "\n",
        "![alt text](https://i.imgur.com/WBGzkF2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B_Hz35kGq5mz",
        "colab": {}
      },
      "source": [
        "x = layers.Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2',\n",
        "           kernel_initializer=kernel_init,\n",
        "           bias_initializer=bias_init)(input_layer)\n",
        "\n",
        "x = layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
        "x = layers.Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
        "x = layers.Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
        "x = layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
        "\n",
        "# Our first inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=64,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=128,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=32,\n",
        "                     filters_pool_proj=32,\n",
        "                     name='inception_3a')\n",
        "\n",
        "# Our second inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=192,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_3b')\n",
        "\n",
        "x = layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
        "\n",
        "# Our third inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=192,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=208,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=48,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4a')\n",
        "\n",
        "\n",
        "# Our first auxilliary output path\n",
        "x1 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
        "x1 = layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
        "x1 = layers.Flatten()(x1)\n",
        "x1 = layers.Dense(1024, activation='relu')(x1)\n",
        "x1 = layers.Dropout(0.7)(x1) # 70% is intentional, as that's stated in the paper.\n",
        "x1 = layers.Dense(10, activation='softmax', name='auxilliary_output_1')(x1)\n",
        "\n",
        "# Our fourth inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=160,\n",
        "                     filters_3x3_reduce=112,\n",
        "                     filters_3x3=224,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4b')\n",
        "\n",
        "# Our fifth inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=256,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4c')\n",
        "\n",
        "# Our sixth inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=112,\n",
        "                     filters_3x3_reduce=144,\n",
        "                     filters_3x3=288,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4d')\n",
        "\n",
        "# Our second auxilliary output path\n",
        "x2 = layers.AveragePooling2D((5, 5), strides=3)(x)\n",
        "x2 = layers.Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
        "x2 = layers.Flatten()(x2)\n",
        "x2 = layers.Dense(1024, activation='relu')(x2)\n",
        "x2 = layers.Dropout(0.7)(x2)\n",
        "x2 = layers.Dense(10, activation='softmax', name='auxilliary_output_2')(x2)\n",
        "\n",
        "# Our seventh inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_4e')\n",
        "\n",
        "x = layers.MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
        "\n",
        "# Our eight inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5a')\n",
        "\n",
        "# Our ninth and final inception module\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=384,\n",
        "                     filters_3x3_reduce=192,\n",
        "                     filters_3x3=384,\n",
        "                     filters_5x5_reduce=48,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5b')\n",
        "\n",
        "# Our final output path\n",
        "x = layers.GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(10, activation='softmax', name='output')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Feb33ppqGHC",
        "colab": {}
      },
      "source": [
        "model = Model(input_layer, [x, x1, x2], name='inception_v1')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ho8t1nWvtbJ"
      },
      "source": [
        "## Our model summary is in agreement with the topological structure of what is proposed in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kxXFwCVbmCOx"
      },
      "source": [
        "# Task 2: Show and Tell: A Neural Image Caption Generator\n",
        "\n",
        "Automatically describing the content of an image is a fundamental problem in AI that connects computer vision and natural language processing. In this task, we will be looking into how we can use CNNs and RNNs to build an Image Caption Generator. We used the Flickr8k dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DfsBNZqjmCOy",
        "colab": {}
      },
      "source": [
        "#Packages\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from random import shuffle\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing import image, sequence\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LSTM, add, RepeatVector, Activation, TimeDistributed, Bidirectional, Add\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0s42xLAkmCO0",
        "colab": {}
      },
      "source": [
        "train_dir = 'data/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "test_dir = 'data/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "captions_dir = 'data/Flickr8k/Flickr8k_text/Flickr8k.token.txt'\n",
        "all_images_dir = 'data/Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
        "\n",
        "\n",
        "train_image_names = open(train_dir, 'r').read().strip().split(\"\\n\")\n",
        "test_image_names = open(test_dir, 'r').read().strip().split(\"\\n\")\n",
        "all_captions = open(captions_dir, 'r').read().strip().split(\"\\n\")\n",
        "\n",
        "\n",
        "train_image_dir = []\n",
        "for i in train_image_names:\n",
        "    train_image_dir.append(all_images_dir + i)\n",
        "    \n",
        "test_image_dir = []\n",
        "for i in test_image_names:\n",
        "    test_image_dir.append(all_images_dir + i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OwjwkfiBmCO4",
        "colab": {}
      },
      "source": [
        "def train_captions(train_image_names, captions):\n",
        "    t_dict = {}    \n",
        "    for i in tqdm(captions):\n",
        "        imagename, caption = i.split(\"\\t\")  \n",
        "        imagename = imagename[:-2]\n",
        "        if imagename not in train_image_names:           \n",
        "            continue            \n",
        "        if imagename not in t_dict:\n",
        "            t_dict[imagename] = []        \n",
        "        t_dict[imagename].append(caption)        \n",
        "    return t_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIw_9GxymCO6",
        "colab": {}
      },
      "source": [
        "def filter_caption(train_image_names, train_dict):\n",
        "    arr = {}\n",
        "    for name, caption in train_dict.items():\n",
        "        arr[name] = []        \n",
        "        for j in caption:\n",
        "            sentence = \"\"            \n",
        "            for i in j.split(\" \"):\n",
        "                word = i.lower()\n",
        "                word = re.sub(r'[^\\w\\s]','',word)\n",
        "                if len(word)==1 and word!= \"a\":\n",
        "                    continue\n",
        "                if not word.isalpha():\n",
        "                    continue\n",
        "                sentence = sentence + word + \" \"            \n",
        "            arr[name].append(sentence.strip()) \n",
        "    return arr                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2Oi1ybTmCO8",
        "colab": {},
        "outputId": "61611e44-3c03-4b48-9346-78b1fff75f62"
      },
      "source": [
        "train_dict = train_captions(train_image_names, all_captions)\n",
        "train_dict = filter_caption(train_image_names, train_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████| 40460/40460 [00:02<00:00, 17138.92it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6-uT-E5mCPA",
        "colab": {}
      },
      "source": [
        "def shuffle_cap(train_dict):\n",
        "    train_img_cap = []\n",
        "    train_img = []\n",
        "    train_captions = []\n",
        "    for key, value in train_dict.items():\n",
        "        for new in value:\n",
        "            train_img_cap.append(key + \"-\" + new)\n",
        "\n",
        "    train_img_cap_copy = train_img_cap[:] \n",
        "    shuffle(train_img_cap_copy)  \n",
        "\n",
        "    for data in train_img_cap_copy:\n",
        "            i,j = data.split(\"-\")\n",
        "            train_img.append(i)\n",
        "            train_captions.append(\"<start> \" + j + \" <stop>\")\n",
        "\n",
        "    return train_img, train_captions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2K5hPg0qmCPC",
        "colab": {}
      },
      "source": [
        "train_img, train_captions = shuffle_cap(train_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_gcjcZlymCPF",
        "colab": {}
      },
      "source": [
        "def max_caption(train_captions):\n",
        "    size = 0\n",
        "    for i in train_captions:\n",
        "        if len(i.split(\" \")) > size:\n",
        "            size = len(i.split(\" \"))\n",
        "    return size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwZ_Z4HVmCPH",
        "colab": {},
        "outputId": "7f4cb6eb-0daf-4be2-e99f-d9b255f7c756"
      },
      "source": [
        "max_caption_length = max_caption(train_captions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qe-53STZmCPJ",
        "colab": {}
      },
      "source": [
        "def vocabulary(train_captions):\n",
        "    words = []\n",
        "    for captions in train_captions:\n",
        "        for i in captions.split(\" \"):\n",
        "            words.append(i)\n",
        "    words = list(set(words))\n",
        "    return words "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTr4kttJmCPL",
        "colab": {}
      },
      "source": [
        "vocab = vocabulary(train_captions)\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tMA4YDIBmCPN",
        "colab": {}
      },
      "source": [
        "#Using the VGG16\n",
        "def VGG():\n",
        "    model = VGG16(weights='imagenet')    \n",
        "    model_input = model.input\n",
        "    model_output = model.layers[-2].output    \n",
        "    model = Model(inputs = model_input, outputs = model_output)    \n",
        "    return model\n",
        "# Image preprocessing for VGG model\n",
        "\n",
        "def image_proc(path, VGG_model):    \n",
        "    temp = image.load_img(path, target_size=(224, 224))\n",
        "    img = image.img_to_array(temp)\n",
        "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
        "    img /= 255\n",
        "    img -= 0.5\n",
        "    img *= 2    \n",
        "    enc = VGG_model.predict(img)\n",
        "    enc = np.reshape(enc, enc.shape[1])\n",
        "    return enc\n",
        "\n",
        "def encode_image(train_image_dir, VGG_model):\n",
        "    arr = {}    \n",
        "    for path in tqdm(train_image_dir):\n",
        "        image_name = path[len(all_images_dir):]\n",
        "        arr[image_name] = image_proc(path, VGG_model)\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xbbXP50mmCPV",
        "colab": {}
      },
      "source": [
        "def make_index(vocab):\n",
        "    arr = {}\n",
        "    for i, j in enumerate(vocab):\n",
        "        arr[j] = i        \n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4X2-mT4lmCPX",
        "colab": {}
      },
      "source": [
        "index = make_index(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "535Rxq_RmCPa",
        "colab": {},
        "outputId": "1ceb4c70-416c-4359-b540-afe1baacaf40"
      },
      "source": [
        "VGG_model = VGG()\n",
        "encoded_train_image = encode_image(train_image_dir, VGG_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████| 6000/6000 [20:27<00:00,  4.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I3ijkAiZmCPf",
        "colab": {}
      },
      "source": [
        "def generator(train_img, train_captions, encoded_train_image, index, vocab_size, max_caption_length, batch_size = 200):\n",
        "    \n",
        "    temp_cap = []\n",
        "    sequence_cap = []\n",
        "    target_cap = []\n",
        "    encoded_image = []\n",
        "    counter = 0\n",
        "    for i in range(len(train_captions)):        \n",
        "        caption_words = train_captions[i].split(\" \")\n",
        "        counter+=1\n",
        "\n",
        "        for j in range(1, len(caption_words)):            \n",
        "            temp_cap = []\n",
        "            next_word = np.zeros((vocab_size,))\n",
        "\n",
        "            for k in range(j):                \n",
        "                temp_cap.append( index[caption_words[k]] )\n",
        "            \n",
        "            next_word [ index[caption_words[j]] ] = 1            \n",
        "            sequence_cap.append(temp_cap)            \n",
        "            target_cap.append(next_word)            \n",
        "            encoded_image.append(encoded_train_image[ train_img[i] ])\n",
        "    \n",
        "        if counter == batch_size:\n",
        "            encoded_image = np.asarray(encoded_image)\n",
        "            sequence_cap = sequence.pad_sequences(sequence_cap, maxlen = max_caption_length, padding ='post')\n",
        "            target_cap = np.asarray(target_cap)\n",
        "            \n",
        "            yield [[encoded_image, sequence_cap], target_cap]\n",
        "            counter = 0\n",
        "            encoded_image = []\n",
        "            sequence_cap = []\n",
        "            target_cap = []\n",
        "            \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C2n-OP8ImCPg",
        "colab": {},
        "outputId": "67e8ab2e-6d27-45c2-c517-ce2ee1fe78ed"
      },
      "source": [
        "in1 = Input(shape = (4096,))\n",
        "par = Dropout(0.5)(in1)\n",
        "par = Dense(256, activation = 'relu')(par)\n",
        "\n",
        "in2 = Input(shape = (max_caption_length,))\n",
        "cap = Embedding(vocab_size, 256)(in2)\n",
        "cap = Dropout(0.5)(cap)\n",
        "cap = LSTM(256, return_sequences=True)(cap)\n",
        "\n",
        "decode = Bidirectional(LSTM(256, return_sequences=False))(add([par, cap]))\n",
        "out = Dense(vocab_size, activation='softmax')(decode)\n",
        "model = Model(inputs = [in1, in2], outputs = out)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 37)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 37, 256)      1940224     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 37, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 37, 256)      525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 37, 256)      0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 512)          1050624     add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7579)         3888027     bidirectional[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 8,453,019\n",
            "Trainable params: 8,453,019\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yoZC1-mqmCPi",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model.fit_generator(generator(train_img, train_captions, encoded_train_image, index, vocab_size, max_caption_length, batch_size = 100), steps_per_epoch = 300, epochs = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6EyMvHZmCPp",
        "colab": {}
      },
      "source": [
        "#Generating captions starting with \"<start>\" and ending when next word predicted is \"<stop>\" or at max_caption_length\n",
        "def gen_captions(image):    \n",
        "    begin_cap = [\"<start>\"]\n",
        "    while True:\n",
        "        temp_caption = [index[i] for i in begin_cap]\n",
        "        temp_caption = sequence.pad_sequences([temp_caption], maxlen=max_caption_length, padding='post')\n",
        "        encode = encoded_test_image[image[len(all_images_dir):]]\n",
        "        preds = model.predict([np.array([encode]), np.array(temp_caption)])\n",
        "        word_prediction = index_word[np.argmax(preds[0])]\n",
        "        begin_cap.append(word_prediction)\n",
        "        \n",
        "        if word_prediction == \"<stop>\" or len(begin_cap) > max_caption_length:\n",
        "            break\n",
        "\n",
        "    final_caption =  ' '.join(begin_cap[1:-1])\n",
        "    final_caption = final_caption.capitalize() + \".\"\n",
        "    return final_caption"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NfRU4xSumCPs",
        "colab": {}
      },
      "source": [
        "index_word ={}\n",
        "for i,j in index.items():    \n",
        "    index_word[j] = i\n",
        "\n",
        "for i in range(10):\n",
        "  new_image = test_image_names[randint(1, 500)]\n",
        "  new_image = all_images_dir + new_image\n",
        "  print ('Caption:', gen_captions(new_image))\n",
        "  Image.open(new_image)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}